<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alexis Cook</title>
    <description>Deep Learning Professional</description>
    <link>http://localhost:4000//</link>
    <atom:link href="http://localhost:4000//feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 23 Apr 2017 01:55:43 -0400</pubDate>
    <lastBuildDate>Sun, 23 Apr 2017 01:55:43 -0400</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>Training a CNN to Detect Facial Keypoints</title>
        <description>&lt;p&gt;This is a draft :)&lt;/p&gt;

&lt;p&gt;In this blog post, I will show you how to use deep learning to detect facial keypoints in images.  The dataset we’ll use was featured on &lt;a href=&quot;link&quot;&gt;Kaggle&lt;/a&gt; a few months ago, and can be downloaded &lt;a href=&quot;link&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The data is composed of &lt;script type=&quot;math/tex&quot;&gt;96 \times 96&lt;/script&gt; grayscale images of cropped human faces, along with 15 corresponding facial landmarks, reported in (&lt;script type=&quot;math/tex&quot;&gt;x, y&lt;/script&gt;) coordinates.  I’ve visualized some of the data below, where you can see that there are two landmarks per eyebrow (&lt;strong&gt;four&lt;/strong&gt; total), three per eye (&lt;strong&gt;six&lt;/strong&gt; total), &lt;strong&gt;four&lt;/strong&gt; for the mouth, and &lt;strong&gt;one&lt;/strong&gt; for the tip of the nose.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/facial_keypoints.png&quot; alt=&quot;facial keypoints&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The goal is to create an automated system that can learn from the patterns in this data, in order to predict the facial landmarks in new images.&lt;/p&gt;

&lt;p&gt;We’ll train a convolutional neural network (CNN) composed of stacks of layers.  The CNN will take an image as input, and its final layer will output a vector with 30 entries, corresponding to the model’s predicted locations of each of the 15 facial keypoints.  The CNN will contain over 7 million weights that we’ll fit to discovered patterns in the training data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/keypoints_CNN.png&quot; alt=&quot;keypoints CNN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We assess the suitability of any candidate collection of model weights according to the mean squared error (MSE) loss function.  MSE is a function of the model weights and is calculated as follows:&lt;/p&gt;

&lt;p&gt;(write as a function of the model weights)&lt;/p&gt;

&lt;p&gt;where x is blah and y is blah.  MSE is minimized when the weights yield a model with predictions that line up well with the true values of the facial keypoints.  Thus, during the training process, we’ll search for the weights that minimize the MSE.&lt;/p&gt;

&lt;p&gt;There are many different optimizers in Keras that can be used to find the weights to minimize MSE; I use stochastic gradient descent, but feel free to experiment with the &lt;a href=&quot;link&quot;&gt;others&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Once we have the weights, we’ll use them to predict the locations of the facial landmarks on new images!  The &lt;a href=&quot;link&quot;&gt;code&lt;/a&gt; in the repository should just be used as a starting point, to get you up and running.  After all, I only run the notebook for 5 epochs.  In practice, you’ll have to train the network for much longer.&lt;/p&gt;

&lt;p&gt;I am a &lt;strong&gt;huge advocate&lt;/strong&gt; of constructing an end-to-end pipeline before engaging in extensive hyperparameter tuning.  Don’t limit yourself to this clean dataset of cropped grayscale faces!  Wouldn’t it be much more &lt;em&gt;rewarding&lt;/em&gt; to build an algorithm that could extract meaningful information from uncropped color images, or (if you’re feeling slightly more ambitious ;)) your webcam?&lt;/p&gt;

&lt;p&gt;In this case, the pipeline could work something like this:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Accept a color image.&lt;/li&gt;
  &lt;li&gt;Convert the image to grayscale.&lt;/li&gt;
  &lt;li&gt;Detect and crop the face contained in the image.&lt;/li&gt;
  &lt;li&gt;Locate the facial keypoints in the cropped image.&lt;/li&gt;
  &lt;li&gt;Overlay the facial keypoints in the original (color, uncropped) image.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The CNN really only gives you Step 4, but the others are quick to fill in with OpenCV.  Steps 1-2 are quick one-liners, and Steps 3-5 use a pre-trained Haar cascade classifier that is easily accessed &lt;a href=&quot;link&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’ve added this code to the &lt;a href=&quot;link&quot;&gt;repository&lt;/a&gt;, and I hope you enjoy!  Please let me know if you end up using it to add a Snapchat-like filter :).&lt;/p&gt;

&lt;p&gt;To get up-and-running quickly, you can run these commands in the terminal:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;git&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clone&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;repo&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;repo&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;face_keypoints&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

</description>
        <pubDate>Sat, 22 Apr 2017 07:39:23 -0400</pubDate>
        <link>http://localhost:4000//2017/training-a-cnn-to-detect-facial-keypoints/</link>
        <guid isPermaLink="true">http://localhost:4000//2017/training-a-cnn-to-detect-facial-keypoints/</guid>
        
        <category>keras</category>
        
        <category>regression</category>
        
        
        <category>keras</category>
        
      </item>
    
      <item>
        <title>Global Average Pooling Layers for Object Localization</title>
        <description>&lt;p&gt;For image classification tasks, a common choice for CNN architecture is repeated blocks of convolution and maxpooling layers, followed by two or more densely connected layers.  The final dense layer has a softmax activation function and a node for each potential object category.&lt;/p&gt;

&lt;p&gt;As an example, consider the VGG-16 model architecture, depicted in the figure below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/vgg16.png&quot; alt=&quot;vgg-16 model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can also summarize the layers of the VGG-16 model by executing the following line of code in the terminal:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'from keras.applications.vgg16 import VGG16; VGG16().summary()'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Your output should appear as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/vgg16_keras.png&quot; alt=&quot;vgg-16 layers in Keras&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You will notice five blocks of (two to three) convolutional layers followed by a max pooling layer.  The final max pooling layer is then flattened and followed by three densely connected layers.  Notice that most of the parameters in the model belong to the fully connected layers!&lt;/p&gt;

&lt;p&gt;As you can probably imagine, an architecture like this has the risk of overfitting to the training dataset.  In practice, judicious use of dropout laters is used to avoid overfitting.&lt;/p&gt;

&lt;p&gt;In the last few years, experts have used global average pooling (GAP) layers to reduce the total number of parameters in the model.  The &lt;a href=&quot;https://arxiv.org/pdf/1312.4400.pdf&quot;&gt;first paper&lt;/a&gt; to propose GAP layers designed an architecture where the final max pooling layer contained one activation map for each image category in the dataset.  The max pooling layer was then fed to a GAP layer, which yielded a vector with a single entry for each possible object in the classification task.  The authors then applied a softmax activation function to yield the predicted probability of each class.  If you peek at the &lt;a href=&quot;https://arxiv.org/pdf/1312.4400.pdf&quot;&gt;original paper&lt;/a&gt;, I especially recommend checking out Section 3.2, titled “Global Average Pooling”.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006&quot;&gt;ResNet-50 model&lt;/a&gt; takes a less extreme approach; instead of getting rid of dense layers altogether, its final convolutional layer is fed to a GAP layer, followed by one densely connected layer with a softmax activation function that yields the predicted object classes.&lt;/p&gt;

&lt;p&gt;In mid-2016, &lt;a href=&quot;http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf&quot;&gt;researchers at MIT&lt;/a&gt; demonstrated that CNNs with GAP layers (a.k.a. GAP-CNNs) that have been trained for a classification task can also be used for &lt;a href=&quot;https://www.youtube.com/watch?v=fZvOy0VXWAI&quot;&gt;object localization&lt;/a&gt;.  That is, a GAP-CNN not only tells us &lt;em&gt;what&lt;/em&gt; object is contained in the image - it also tells us &lt;em&gt;where&lt;/em&gt; the object is in the image, and through no additional work on our part!  The localization is expressed as a heat map (henceforth referred to as a &lt;strong&gt;class activation map&lt;/strong&gt;), where the color-coding scheme identifies regions that are relatively important for the GAP-CNN to perform the object identification task.  Please check out the YouTube video below for an &lt;em&gt;awesome&lt;/em&gt; demo!&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; style=&quot;padding:0px 0px 20px 0px;&quot; src=&quot;https://www.youtube.com/embed/fZvOy0VXWAI?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;In this blog post, we explore the localization ability of the pre-trained ResNet-50 model, using the technique from &lt;a href=&quot;http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf&quot;&gt;this paper&lt;/a&gt;.  The main idea is that each of the activation maps in the final convolutional layer acts as a detector for a different pattern in the image, localized in space.  To get the &lt;em&gt;class&lt;/em&gt; activation map corresponding to an image, we need only to translate these detected patterns to detected objects.  This translation is done by noticing each node in the GAP layer corresponds to a different activation map, and that the weights in the final dense layer encode each activation map’s contribution to the predicted object class.  We can thus sum the contributions of each of the detected patterns in the activation maps (from the final convolutional layer), where detected patterns that are more important to the predicted object class are given more weight.&lt;/p&gt;

&lt;p&gt;Now that we’ve explored the intuition, we’re ready for a concise description of how the code operates.  Let &lt;script type=&quot;math/tex&quot;&gt;f_k&lt;/script&gt; represent the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;-th activation map in the final convolutional layer.  In the pre-trained ResNet-50 model, the last convolutional layer contains 2048 activation maps, each 7 pixels high and 7 pixels wide.  So, &lt;script type=&quot;math/tex&quot;&gt;f_0&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;7\times7&lt;/script&gt; pixels and is the first activation map in the convolutional layer, &lt;script type=&quot;math/tex&quot;&gt;f_1&lt;/script&gt; is also &lt;script type=&quot;math/tex&quot;&gt;7\times7&lt;/script&gt; pixels and is the second activation map, and so on, where &lt;script type=&quot;math/tex&quot;&gt;f_{2047}&lt;/script&gt; is likewise &lt;script type=&quot;math/tex&quot;&gt;7\times7&lt;/script&gt; pixels and is the final activation map.  In order to permit comparison to the original image, &lt;a href=&quot;https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.ndimage.zoom.html#scipy.ndimage.zoom&quot;&gt;bilinear upsampling&lt;/a&gt; is used to resize each activation map to &lt;script type=&quot;math/tex&quot;&gt;224 \times 224&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Next, we look at the class that is predicted by the model.  The output node corresponding to the predicted class is connected to every node in the GAP layer.  Let &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; represent the weight connecting the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;-th node in the GAP layer to the output node corresponding to the predicted dog breed.  Then, in order to obtain the class activation map, we need only compute the sum&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;w_0 \cdot f_0 + w_1 \cdot f_1 + \ldots + w_{2047} \cdot f_{2047}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;This sum is a &lt;script type=&quot;math/tex&quot;&gt;224\times 224&lt;/script&gt; array that is then plotted in the code to produce the class activation map.  If you’d like to use this code to do your own object localization, you need only download the repository.&lt;/p&gt;

&lt;p&gt;NOTE: This blog is still in draft mode; the math is not rendering.  Should be fixed soon!&lt;/p&gt;

&lt;p&gt;To get up-and-running even faster, you can run these commands in the terminal:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;git&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clone&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;repo&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;repo&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet50_localization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 09 Apr 2017 07:39:23 -0400</pubDate>
        <link>http://localhost:4000//2017/global-average-pooling-layers-for-object-localization/</link>
        <guid isPermaLink="true">http://localhost:4000//2017/global-average-pooling-layers-for-object-localization/</guid>
        
        <category>keras</category>
        
        <category>localization</category>
        
        
        <category>keras</category>
        
      </item>
    
      <item>
        <title>Using Transfer Learning to Classify Images with Keras</title>
        <description>&lt;p&gt;In this blog post, I will show you how to &lt;em&gt;efficiently&lt;/em&gt; use deep learning to train an algorithm to perform object classification.  This blog post is inspired by a &lt;a href=&quot;https://medium.com/@st553/using-transfer-learning-to-classify-images-with-tensorflow-b0f3142b9366&quot;&gt;recent Medium post&lt;/a&gt; that made use of Tensorflow.  I will adapt the code to Keras (version 2.0.2), and all code will be written in Python 3.5.&lt;/p&gt;

&lt;p&gt;I will assume that you are already familiar with the ideas behind convolutional neural networks (CNNs) and transfer learning, and we’ll focus on discussing the details of my code in Keras.&lt;/p&gt;

&lt;p&gt;If you need to learn more about CNNs, I recommend reading the notes for the &lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;CS231n&lt;/a&gt; course at Stanford.  All lectures are also available &lt;a href=&quot;https://www.youtube.com/watch?v=LxfUGhug-iQ&amp;amp;list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&amp;amp;index=7&quot;&gt;online&lt;/a&gt;.  You are also encouraged to check out Term 2 of Udacity’s &lt;a href=&quot;https://www.udacity.com/course/artificial-intelligence-nanodegree--nd889&quot;&gt;Artificial Intelligence Nanodegree&lt;/a&gt;, where you can find a comprehensive introduction to neural networks (NNs), CNNs (including transfer learning), and recurrent neural networks (RNNs).&lt;/p&gt;

&lt;h4 id=&quot;the-dataset&quot;&gt;The Dataset&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cs.toronto.edu/~kriz/cifar.html&quot;&gt;CIFAR-10&lt;/a&gt; is a popular dataset composed of 60,000 tiny color images that each depict an object from one of ten different categories.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/cifar10.png&quot; alt=&quot;cifar-10 dataset&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This &lt;a href=&quot;https://keras.io/datasets/&quot;&gt;dataset&lt;/a&gt; is simple to load in Keras.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cifar10&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cifar10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;extracting-the-inceptionv3-bottleneck-features&quot;&gt;Extracting the InceptionV3 Bottleneck Features&lt;/h4&gt;

&lt;p&gt;We won’t build or train our own CNNs.  Instead, we will use &lt;strong&gt;transfer learning&lt;/strong&gt; to leverage a pre-trained CNN that has demonstrated state-of-the-art performance in object classification tasks.&lt;/p&gt;

&lt;p&gt;Keras makes it very easy to access several pre-trained &lt;a href=&quot;https://keras.io/applications/&quot;&gt;CNN architectures&lt;/a&gt;.  For now, we will focus on the InceptionV3 architecture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/inception.png&quot; alt=&quot;inception architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After importing the necessary Python class, it’s only one line of code to get the model, along with the pre-trained weights.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.applications.inception_v3&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;InceptionV3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;base_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;InceptionV3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'imagenet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;include_top&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The pre-trained InceptionV3 architecture is now stored in the variable &lt;code class=&quot;highlighter-rouge&quot;&gt;base_model&lt;/code&gt;.  The final layer of this network is a densely connected layer designed to distinguish between the &lt;a href=&quot;https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a&quot;&gt;1000 different object categories&lt;/a&gt; in the ImageNet database.  We will remove this final layer and save the resultant network in a new model.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'avg_pool'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This new model will no longer return a predicted image class, since the classification layer has been removed; however, the CNN now stored in &lt;code class=&quot;highlighter-rouge&quot;&gt;model&lt;/code&gt; still provides us with a useful way to extract features from images.  By passing each of the CIFAR-10 images through this model, we can convert each image from its 32x32x3 array of raw image pixels to a vector with 2048 entries.  In practice, we refer to this dataset of 2048-dimensional points as InceptionV3 bottleneck features.&lt;/p&gt;

&lt;h4 id=&quot;using-t-sne-to-visualize-bottleneck-features&quot;&gt;Using t-SNE to Visualize Bottleneck Features&lt;/h4&gt;

&lt;p&gt;Towards visualizing the bottleneck features, we will use a dimensionality reduction technique called &lt;a href=&quot;http://distill.pub/2016/misread-tsne/&quot;&gt;t-SNE&lt;/a&gt; (aka t-Distributed Stochastic Neighbor Embedding).  t-SNE reduces the dimensionality of each point, in a way where the points in the lower-dimensional space preserve the pointwise distances from the original, higher-dimensional space.  Scikit-learn &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html&quot;&gt;has an implementation&lt;/a&gt; of t-SNE, but it is too slow for our purposes.  Instead, to work with our very large, highly dimensional dataset, you’ll need to work with a faster implementation.  The implementation we encourage you to use can be found &lt;a href=&quot;https://github.com/alexisbcook/tsne&quot;&gt;on github&lt;/a&gt;, and can be installed by running &lt;code class=&quot;highlighter-rouge&quot;&gt;pip install git+https://github.com/alexisbcook/tsne.git&lt;/code&gt; in the terminal.&lt;/p&gt;

&lt;p&gt;After plotting the resulting 2-dimensional points, color-coded by label, we get the plot below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/tsne.png&quot; alt=&quot;t-sne plot for transfer learning on cifar-10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;InceptionV3 does an amazing job with teasing out the content in the image, where points containing similar objects are mostly confined to nearby regions in the 2D plot.&lt;/p&gt;

&lt;h4 id=&quot;performing-classification-with-transfer-learning&quot;&gt;Performing Classification with Transfer Learning&lt;/h4&gt;

&lt;p&gt;When we train a very shallow NN on the bottleneck features, we attain a test accuracy of 80 percent!  That’s amazing! :)&lt;/p&gt;

&lt;h4 id=&quot;play-with-the-code&quot;&gt;Play with the Code!&lt;/h4&gt;

&lt;p&gt;Can we do better with other pre-trained architectures?  Feel free to download the code on Github and try your own hand at transfer learning!  &lt;strong&gt;Link to repository coming soon ~&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 08 Apr 2017 07:39:23 -0400</pubDate>
        <link>http://localhost:4000//2017/using-transfer-learning-to-classify-images-with-keras/</link>
        <guid isPermaLink="true">http://localhost:4000//2017/using-transfer-learning-to-classify-images-with-keras/</guid>
        
        <category>keras</category>
        
        <category>classification</category>
        
        <category>transfer-learning</category>
        
        
        <category>keras</category>
        
      </item>
    
  </channel>
</rss>
